{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ac142-e5d5-4ea6-b169-0e6fed1b6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is ensemble learning :\n",
    "\n",
    "# Ensemble learning is a machine learning technique where multiple models are \n",
    "# combined to improve overall predictive performance and generalization\n",
    "\n",
    "# Bagging (Bootstrap Aggregating): It involves training multiple instances of the same model on different subsets\n",
    "# of the training data and combining their predictions. Random Forest is a common example of bagging.\n",
    "\n",
    "# Boosting: Boosting focuses on sequentially training models, where each subsequent model\n",
    "# corrects the errors of the previous ones. Examples include AdaBoost, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710e978-1814-4a47-943b-ef3008911aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is OOB Score ?  \n",
    "\n",
    "# The OOB score is the performance metric (e.g., accuracy, MSE) computed on these out-of-bag samples. \n",
    "# It serves as a validation measure for the model without requiring a separate validation set. \n",
    "# OOB scores help assess the model's performance and can be used for early stopping or hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc77bfd6-f92e-4b4b-9f5c-6404ebe734cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9486081370449679\n"
     ]
    }
   ],
   "source": [
    "# What is R2 Square ?\n",
    "# R-squared, also known as the coefficient of determination, measures the proportion of the variance in the \n",
    "# dependent variable that is explained by the independent variables in a regression model. It ranges from 0 to 1, \n",
    "# with higher values indicating a better fit of the model to the data.\n",
    "\n",
    "\n",
    "# R2 = 1âˆ’ Sum of Squared Residuals/Total Sum of Squares\n",
    "\n",
    " \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05facbd9-3078-43c6-a6ff-efdcf8c11713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Adjusted R2 square ?\n",
    "# Adjusted R-squared is a modified version of R-squared that takes into account the number of \n",
    "# independent variables in the model. \n",
    "# It penalizes the addition of unnecessary variables that may not contribute significantly \n",
    "# to the model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6712006-5b00-474a-b0d0-269fc69636ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised and unupervised ?\n",
    "\n",
    "# Supervised learning is a type of machine learning where the algorithm is trained on labeled data, \n",
    "# meaning the input data is paired with the correct output or target. \n",
    "# The algorithm learns to map inputs to outputs by finding patterns in the training data. \n",
    "# The goal is to make accurate predictions or classifications on new, unseen data.\n",
    "\n",
    "\n",
    "# Unsupervised learning is a type of machine learning where the algorithm is trained on unlabeled data, \n",
    "# meaning there are no predefined outputs or targets. The algorithm's objective is to discover inherent patterns, \n",
    "# structures, or relationships within the data. It's often used for clustering or dimensionality reduction tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc068a78-d9a7-4b89-b05f-e54b90ecd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Unsupervised technique (there are many but here is few for revision)\n",
    "\n",
    "Clustering:\n",
    "# Clustering methods group similar data points together into clusters based on certain similarity measures. \n",
    "# The goal is to discover natural groupings within the data.\n",
    "\n",
    "# K-Means Clustering\n",
    "# Hierarchical Clustering\n",
    "# DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "# Gaussian Mixture Models (GMM)\n",
    "\n",
    "# Dimensionality Reduction:\n",
    "# Dimensionality reduction techniques aim to reduce the number of features while retaining as much \n",
    "# relevant information as possible. \n",
    "# They are useful for visualizing high-dimensional data or improving model efficiency.\n",
    "\n",
    "# Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06948030-37b2-40ad-89a8-c368efabe6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clusters data into a predetermined number of clusters by minimizing the sum of squared distances \n",
    "#between data points and their cluster centers.\n",
    "# Real Case Use: Customer segmentation in marketing for identifying distinct customer groups based on purchase behaviors.\n",
    "\n",
    "# Hierarchical Clustering:\n",
    "# Hierarchical Clustering builds a tree-like structure of nested clusters, allowing for various levels of granularity in cluster assignments.\n",
    "# Real Case Use: Biological taxonomy to classify species into hierarchical categories like kingdom, phylum, etc.\n",
    "\n",
    "# DBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n",
    "# DBSCAN groups dense regions of data points and identifies noise points as outliers, without requiring the number of clusters as input.\n",
    "# Real Case Use: Identifying hotspots of criminal activities in crime analysis.\n",
    "\n",
    "# Gaussian Mixture Models (GMM):\n",
    "# GMM models data as a mixture of multiple Gaussian distributions, enabling probabilistic assignment of data points to clusters.\n",
    "# Real Case Use: Image segmentation for separating foreground and background objects.\n",
    "\n",
    "# Principal Component Analysis (PCA):\n",
    "# PCA reduces the dimensionality of data by projecting it onto a new orthogonal basis that captures the most important variability.\n",
    "# Real Case Use: Reducing the number of features in facial recognition to improve computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1415f-d003-45d0-98ee-f9467514bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In short Time Series Will Do One to Understand Better \n",
    "\n",
    "# So what is time series : \n",
    "# 1. Time Series Data: Time series data is a sequence of data points collected over a specific time interval. \n",
    "# It's characterized by its order, which is usually chronological. \n",
    "# Components of time series include trend (long-term movement), seasonality (repeating patterns), and noise (random fluctuations).\n",
    "\n",
    "\n",
    "# 2. Data Preprocessing:\n",
    "# Handling Missing Data: You might interpolate missing values or fill them using methods like forward fill or backward fill.\n",
    "# Dealing with Outliers: Outliers can distort analysis and predictions. You may remove or transform them.\n",
    "# Resampling: You can change the frequency of data by upsampling (increasing frequency) or downsampling (decreasing frequency).\n",
    "# Handling Time Zones and Irregular Intervals: Ensure data is consistently timestamped and aligned.\n",
    "\n",
    "\n",
    "# 3. Time Series Visualization:\n",
    "# Line Plots and Area Plots: Plotting the raw time series data helps visualize trends and patterns.\n",
    "# Seasonal Decomposition: Using decomposition, you can separate a time series into trend, seasonality, and residual components.\n",
    "# Autocorrelation and Partial Autocorrelation Plots (ACF and PACF): These help identify patterns in the data that can guide model selection.\n",
    "\n",
    "\n",
    "# 4. Time Series Decomposition:\n",
    "# Trend Extraction: Removing the trend helps in studying seasonality and residual patterns.\n",
    "# Seasonal Component Extraction: Isolating seasonality reveals recurring patterns within a given period.\n",
    "# Residual Analysis: Analyzing residuals helps identify any leftover patterns not captured by trend and seasonality.\n",
    "\n",
    "\n",
    "# 5. Time Series Forecasting:\n",
    "# Stationarity and Differencing: Making a time series stationary by removing trend and seasonality can improve forecast accuracy.\n",
    "# ARIMA Models: AutoRegressive Integrated Moving Average models capture the relationship between current and past observations, \n",
    "# and their differences.\n",
    "# SARIMA Models: Seasonal ARIMA models handle seasonality along with ARIMA components.\n",
    "# Exponential Smoothing Methods (Holt-Winters): These models assign exponentially decreasing weights to past observations.\n",
    "\n",
    "\n",
    "# 6. Evaluation Metrics:\n",
    "# Mean Absolute Error (MAE): Average of absolute errors.\n",
    "# Mean Squared Error (MSE): Average of squared errors.\n",
    "# Root Mean Squared Error (RMSE): Square root of MSE.\n",
    "# Mean Absolute Percentage Error (MAPE): Percentage of average absolute errors.\n",
    "\n",
    "# 7. Feature Engineering:\n",
    "# Lag Features: Using past observations as features for prediction.\n",
    "# Rolling Window Statistics: Using rolling averages or other statistics over a window of time.\n",
    "# Seasonal Features: Incorporating cyclical patterns as features.\n",
    "\n",
    "# 8. Model Selection and Tuning:\n",
    "# Cross-Validation: Splitting data into training and validation sets to evaluate models.\n",
    "# Hyperparameter Tuning: Adjusting model parameters to optimize performance.\n",
    "# Grid Search and Random Search: Methods to systematically find optimal hyperparameters.\n",
    "\n",
    "# 9. Best Practices:\n",
    "# Handling Overfitting: Regularization techniques to prevent models from fitting noise.\n",
    "# Understanding Model Limitations: Time series models have assumptions and limitations; consider these in interpretation.\n",
    "# Monitoring and Updating Forecasts: Continuous monitoring and model updates based on new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21876490-9232-4a15-a567-a99fc7aeccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our Main Algorithms : \n",
    "\n",
    "# Linear Reg \n",
    "# Linear regression is a simple machine learning algorithm used for predicting a \n",
    "# continuous outcome based on one or more input features. It assumes a linear relationship between \n",
    "# the input variables and the target variable and calculates the best-fitting \n",
    "# line to minimize the difference between predicted and actual values.\n",
    "\n",
    "# MAE (Mean Absolute Error):\n",
    "# MAE measures the average absolute difference between predicted and actual values. \n",
    "# It's less sensitive to outliers compared to other metrics, making it suitable for cases where large errors shouldn't be heavily penalized.\n",
    "\n",
    "# MSE (Mean Squared Error):\n",
    "# MSE calculates the average of squared differences between predicted and actual values.\n",
    "# It amplifies larger errors and is commonly used in regression tasks.\n",
    "\n",
    "# RMSE (Root Mean Squared Error):\n",
    "# RMSE is the square root of MSE. It has the same unit as the target variable, making it more interpretable.\n",
    "# RMSE is a widely used evaluation metric, especially in regression problems.\n",
    "\n",
    "#These are regularization techniques used to prevent overfitting in regression models.\n",
    "# Ridge adds a penalty term to the model's coefficients, \n",
    "# Lasso encourages sparsity by forcing some coefficients to become exactly zero, and \n",
    "# Elastic Net combines both Lasso and Ridge penalties. \n",
    "# These techniques help improve model generalization and stability.\n",
    "\n",
    "# ----\n",
    "# Regularization techniques like Ridge, Lasso, and Elastic Net help \n",
    "# prevent overfitting by adding constraints to model parameters. They penalize \n",
    "# large parameter values, making the model less sensitive to noise in the data. \n",
    "# This prevents the model from fitting noise and improves its generalization to new data.\n",
    "\n",
    "# For feature selection, Lasso and Elastic Net are particularly useful. \n",
    "# Lasso sets some coefficients to zero, effectively removing corresponding features from the model.\n",
    "# This helps in identifying the most important features. \n",
    "# Ridge and Elastic Net also downscale less relevant features, reducing their impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccadc2eb-1b7f-4d9a-9ba6-3f5f594aeb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 13, 22, 32, 35]\n",
      "[2, 32]\n"
     ]
    }
   ],
   "source": [
    "arr1 = [1,2,32,13]\n",
    "arr2 = [2,3,22,32,35]\n",
    "\n",
    "union = sorted(set(arr1 + arr2))\n",
    "print(union)\n",
    "\n",
    "intersection = [x for x in arr1 if x in arr2]\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3dc443-0c15-4db8-8bd7-3238e80735a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression:\n",
    "# Logistic Regression is a classification algorithm used to predict the probability of an\n",
    "# instance belonging to a particular class. It models the relationship between input \n",
    "# features and the likelihood of a binary outcome using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e311c4-4659-441c-892f-5db41f440815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes:\n",
    "# Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. \n",
    "# It assumes that features are conditionally independent given the class label, \n",
    "# which simplifies calculations. It's particularly effective for text classification \n",
    "# and works well with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7bac7-e85d-4eb3-b905-1fbe31b75adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN (K-Nearest Neighbors):\n",
    "# KNN is a simple classification and regression algorithm. It assigns a label to a new \n",
    "# instance based on the majority class of its k nearest neighbors in the feature space. \n",
    "# It's non-parametric and easy to understand, making it suitable for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388da5fa-fd50-46db-b272-c5877dae6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting/Underfitting/Bias/Variance:\n",
    "# Overfitting occurs when a model learns the training data too well, including noise, leading to poor generalization.\n",
    "# Underfitting happens when a model is too simple to capture the underlying patterns. \n",
    "# Bias refers to the error due to the model's assumptions.\n",
    "# Variance is the model's sensitivity to variations in the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee2faa-5681-4d15-b984-960d463c06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM (Support Vector Machine):\n",
    "# SVM is a powerful classification algorithm that finds a hyperplane in a high-dimensional space \n",
    "# to best separate different classes.\n",
    "# It aims to maximize the margin between classes. \n",
    "# In regression tasks, SVM becomes SVR (Support Vector Regression) by fitting a \n",
    "# hyperplane to minimize the error within a certain margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea7981-bd4d-4723-8ef4-36f684b69cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Kernel:\n",
    "# SVM kernels transform data into higher-dimensional space, \n",
    "# allowing complex nonlinear boundaries to be drawn. For example, \n",
    "# the Gaussian (RBF) kernel calculates similarity based on distance between points in higher dimensions.\n",
    "\n",
    "# Example: In text classification, an RBF kernel can project text data \n",
    "# into a higher-dimensional space to classify documents based on their semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7090a0-c367-40bb-b646-618814ef37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree:\n",
    "# Decision Trees recursively split data based on features to create a tree-like structure. \n",
    "# It's used for both classification and regression tasks. \n",
    "# It makes decisions at each node by maximizing information gain or minimizing impurity.\n",
    "# Formula: Information Gain = Entropy(parent) - Weighted Avg. Entropy(children)\n",
    "# Example: In a credit risk assessment, a decision tree may split customers based on credit score, \n",
    "# income, and other features to determine loan approval.\n",
    "\n",
    "# Post-Pruning and Pre-Pruning:\n",
    "# Post-pruning involves growing a full decision tree and then removing nodes that\n",
    "# provide little predictive power. \n",
    "# Pre-pruning, on the other hand, involves setting constraints \n",
    "# while growing the tree to limit depth, splits, or minimum samples per leaf.\n",
    "# Example of Post-Pruning: After building a deep tree for email classification, \n",
    "# prune nodes with insignificant feature splits.\n",
    "# Example of Pre-Pruning: Set a limit on tree depth to prevent overfitting.\n",
    "\n",
    "\n",
    "# Variance Reduction in Decision Trees:\n",
    "# Decision Trees can lead to overfitting due to high variance. \n",
    "# Techniques like pruning, limiting depth, or setting minimum samples \n",
    "# per leaf reduce variance by simplifying the tree's complexity.\n",
    "# Example: In predicting housing prices, reducing the maximum depth of the \n",
    "# tree prevents overfitting by avoiding complex splits for small subsets of data.\n",
    "\n",
    "\n",
    "# Decision trees use various methods to find purity in splits and determine the best features for splitting:\n",
    "# Gini Impurity: It measures the frequency at which a randomly selected element would be incorrectly classified.\n",
    "# A lower Gini score indicates a purer split.\n",
    "\n",
    "# Entropy: Entropy measures the impurity or randomness in a set. It is used to quantify the uncertainty of a split. \n",
    "# A lower entropy indicates a more certain split.\n",
    "\n",
    "# Information Gain: It measures the reduction in entropy or impurity achieved by a particular split. \n",
    "# Features that lead to the most information gain are chosen as the best splitting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b37bea-24d7-41c3-8232-63de3918708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data, Test Data, Validation Data:\n",
    "\n",
    "# Training Data: The dataset used to train a machine learning model.\n",
    "# It's used to learn the patterns and relationships between input features and target labels.\n",
    "\n",
    "# Test Data: A separate dataset used to evaluate the model's performance after training. \n",
    "# It helps to estimate how well the model will generalize to new, unseen data.\n",
    "\n",
    "# Validation Data: An optional dataset used during training to tune hyperparameters and prevent overfitting.\n",
    "# It's not used for model evaluation like test data.\n",
    "#-----------------------------------------------------------------\n",
    "# Cross Validation:\n",
    "# A technique to assess the model's performance by splitting the dataset into multiple subsets, \n",
    "# training and evaluating the model on different combinations of these subsets.\n",
    "\n",
    "# Types of Cross Validation:\n",
    "\n",
    "# K-Fold Cross Validation: Data is divided into k subsets. The model is trained k times,\n",
    "# each time using k-1 subsets for training and 1 subset for validation.\n",
    "\n",
    "# Stratified K-Fold Cross Validation: Similar to k-fold, but ensures each \n",
    "# fold has a similar distribution of target labels.\n",
    "\n",
    "# Leave-One-Out Cross Validation (LOOCV): Each observation is used as a validation set,\n",
    "# and the rest are used for training.\n",
    "\n",
    "# Time Series Cross Validation: Maintains temporal order, where earlier data is used for \n",
    "# training and later data for validation.\n",
    "\n",
    "# Repeated Cross Validation: K-Fold process is repeated multiple times and results are averaged.\n",
    "#-------------------------------------------------------\n",
    "# Grid Search and Random Search:\n",
    "# Both are hyperparameter tuning techniques.\n",
    "\n",
    "# Grid Search: Exhaustively searches through a predefined hyperparameter grid to find \n",
    "# the best combination of hyperparameters.\n",
    "# Random Search: Randomly samples from the hyperparameter space, allowing more efficient exploration.\n",
    "\n",
    "# Other Hyperparameter Tuning Techniques:\n",
    "# Bayesian Optimization: Models the function relating hyperparameters to performance and chooses next values to evaluate.\n",
    "# Genetic Algorithms: Evolves a population of hyperparameter combinations over several generations.\n",
    "# Optuna, Hyperopt: Libraries that automate hyperparameter optimization using various strategies.\n",
    "# These techniques ensure models are well-tuned and evaluated to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2aebe7-6b35-4702-a64f-44f4219da4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
